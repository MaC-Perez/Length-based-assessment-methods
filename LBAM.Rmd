---
title: "LENGTH BASED ASESSMENT METHODS Cristina_Perez"
output:
  pdf_document: default
  html_document: default
date: "2022-12-16"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


To reproduce example from class 2

Exercise 1: von Bertalanffy growth and Beverton-Holt length-based catch curve
- Yellow Perch example

```{r setup, include=FALSE}
library(TropFishR)
library(fishmethods)
library(MLZ)
library(TMB)
library(dplyr)
```

estimate vonB length at age growth model parameters in R using
the function “growth” from fishmethods. (Yellow Pearch example)


```{r setup}
YPdat=read.csv("examples/class 2/YPlens.csv")
colnames(YPdat)=c("age","length")
growmod=growth(intype=1,unit=1,size=YPdat$length,age=YPdat$age,
               calctype=1,wgtby=1,error=1,Sinf=300,K=0.3,t0=-1)
growmod$vout
```


Nonlinear regression model
model: size ~ Sinf * (1 - exp(-(K * (age - t0))))
data: x
Sinf        K       t0 
315.8946   0.2678  -1.5118 
residual sum-of-squares: 2503805

Number of iterations to convergence: 3 
Achieved convergence tolerance: 3.543e-08

Exercise 2: Estimation of Z using Beverton-Holt method and simulated lengthfrequency
data in TropFishR package. Estimate Z for all 3 years using the Z_BevertonHolt function.

```{r setup}
data(synLFQ2)
synLFQ2

Z1960=Z_BevertonHolt(synLFQ2, catch_columns = 1, Lprime_tprime = 47.5)
Z1970=Z_BevertonHolt(synLFQ2, catch_columns = 2, Lprime_tprime = 47.5)
Z1980=Z_BevertonHolt(synLFQ2, catch_columns = 3, Lprime_tprime = 47.5)
Z1960$Z
Z1970$Z
Z1980$Z
BHests=c(Z1960$Z,Z1970$Z,Z1980$Z)
BHests
```

BHests
[1] 0.5949557 0.8872215 1.1724409

Exercise 3: Estimation of Z using Powell-Wetherall method and simulated
length-frequency data in TropFishR package

```{r setup}
synLFQ2mod=synLFQ2[-c(1,5)]
synLFQ2mod$t0=0
synLFQ2mod$catch=synLFQ2$catch[]
dev.new()
PW1960=powell_wetherall(synLFQ2mod,catch_columns=1)
PW1970=powell_wetherall(synLFQ2mod,catch_columns=2)
PW1980=powell_wetherall(synLFQ2mod,catch_columns=3)
PWests=c(PW1960$ZK*PW1960$K,PW1970$ZK*PW1970$K,PW1980$ZK*PW1980$K)
```

PWests
[1] 0.6010913 0.9221825 1.2687794

Exercise 4: Estimation of Z using a length-converted catch curve and simulated
length-frequency data in TropFishR package

```{r setup}
LC1960 <- catchCurve(synLFQ2mod,catch_columns=1)
LC1970 <- catchCurve(synLFQ2mod,catch_columns=2)
LC1980 <- catchCurve(synLFQ2mod,catch_columns=3)
LCests=c(LC1960$Z,LC1970$Z,LC1980$Z)
```
LCests
[1] 0.6050052 0.9046315 1.2060942

Exercise 5: Graph all 3 sets of Z estimates
Create an object containing the years and combine Z estimates from all 3 methods in the same object. Plot
them on the same graph.

```{r setup}
yrs=c(1960,1970,1980)
allres=cbind(yrs,BHests,PWests,LCests)
plot(yrs,PWests,type="b",col="blue",ylim=c(0.5,1.5),ylab="Z or relative Z")
lines(yrs,BHests,type="b",col="red",pch="*")
lines(yrs,LCests,type="b",col="black",pch="+")
legend(1960,1.5,legend=c("PW","BH","LC"),
       col=c("blue","red","black"),pch=c("o","*","+"), ncol=1)
```


To reproduce example from class 2

```{r setup}
library(TropFishR)
graphics.off()
rm(list=ls(all=TRUE))

data("alba")
alba
```

Length frequency data set for the mollusc Abra alba. Determine the number of bins to include in your moving average. #TropFishR the default value is 5 but we can specify different values with the argunet MA.

MA should approximate the number of bins spanning the width covered by the smallest
(i.e. youngest) cohort if we check in the data we could use 7 bins 

```{r setup}
# explore different values for MA 
alba <- lfqRestructure(alba, MA=7)
#alba <- lfqRestructure(alba, MA=5)
#alba <- lfqRestructure(alba, MA=3)
plot(alba)
```

Identify appropriate limits for vonB parameters to constrain the search
space in ELEFAN_GA. the lower and upper bounds of Linf and K as an argument when calling the function.
start with Linf .... use a catch curve and the data 

```{r setup}
PW <- powell_wetherall(alba, catch_columns = 1:7)
```

$Linf_est
[1] 11.3413

$se_Linf
[1] 0.9659902

$confidenceInt_Linf
[1]  8.977609 13.704995

$ZK
[1] 0.8569509

$se_ZK
[1] 0.03007039


Compare the Linf and K values you generated with Response Surface Analysis (RSA)
but also we can use response surface analysis 

Linf between 9 and 12 and K between 2 and 3

```{r setup}
alba2 <- ELEFAN(
  lfq = alba, MA = 7,
  Linf_range = seq(7, 20, length.out = 30),
  K_range = exp(seq(log(0.1),log(4), length.out = 30)),
  method = "cross",
  cross.date = alba$dates[3],
  cross.midLength = alba$midLengths[5],
  contour = TRUE, add.values = FALSE,
  hide.progressbar = TRUE # change to 'TRUE' to follow algorithm's progression
)

```

```{r setup}
points(alba2$par["Linf"], alba2$par["K"], pch="*", cex=2, col=2)
unlist(alba2$par)
alba2$Rn_max
plot(alba2)
```
 Linf        K t_anchor        C       ts     phiL 
9.689655 2.400000 0.370000 0.000000 0.000000 2.352828 

Run ELEFAN_GA using your values for constraining Linf and K from Step 4. Do this by changing the Linf and K values in the low_par and up_par arguments.
Generate growth curve parameter value estimates and plot those curves on your restructured data.

$confidenceInt_Linf
[1]  8.977609 13.704995

```{r setup}
Linf=PW$confidenceInt_Linf[1]
Linf=PW$confidenceInt_Linf[2]
```

```{r setup}
set.seed(1)
alba4 <- ELEFAN_GA(
  lfq = alba,
  seasonalised = FALSE,
  low_par = list(Linf=PW$confidenceInt_Linf[1], K=1, t_anchor=0, ts=0, C=0),
  up_par = list(Linf=PW$confidenceInt_Linf[2], K=4, t_anchor=1, ts=1, C=1),
  popSize = 60,
  pmutation = 0.2,
  maxiter = 100,
  run = 20,
  MA = 7,
  plot.score = TRUE,
  monitor = FALSE,
  parallel = FALSE
)
unlist(alba4$par)
#Linf          K   t_anchor       phiL 
#10.1301838  1.9618729  0.2643963  2.3039055 
alba4$Rn_max
plot(alba4)
```

To reproduce example from class 2

```{r setup}
source("examples/class 4/read.report.R")
library(ggplot2)
library(reshape2)
graphics.off()
rm(list=ls(all=TRUE))

shell("examples/class 4/bcrab.exe")
```

```{r setup}
file.rename("examples/class 4/bcrab.rep","bcrab.txt")
repfile=read.delim("examples/class 4/bcrab.txt",header=FALSE,sep=" ")
popmod_ests=repfile[1:31,]
colnames(popmod_ests)=c("Year","Adult_N","Rec_N","u","log_R_devs")
obsmod_ests=repfile[32:62,]
colnames(obsmod_ests)=c("Year","Est_adult","Obs_adult","Est_juv","Obs_juv")
popmod_estsN=melt(popmod_ests[,1:3],id=c("Year"))
compad=melt(obsmod_ests[,1:3],id="Year")
colnames(compad)=c("Year","ObsPred","Est")
compad$ObsPred=factor(compad$ObsPred)
compjuv=melt(obsmod_ests[,c(1,4:5)],id="Year")
colnames(compjuv)=c("Year","ObsPred","Est")
compjuv$ObsPred=factor(compjuv$ObsPred)
catch=read.delim("examples/class 4/bcrab_catch.txt",header=T)
```

```{r setup}
ggplot(data=popmod_ests, aes(x=Year, y=Adult_N)) +
  geom_line()+
  geom_point()+
  labs(x="Year", y = "Adult abundance")+
  theme_classic()
```

```{r setup}
ggplot(data=popmod_ests, aes(x=Year, y=Rec_N)) +
  geom_line()+
  geom_point()+
  labs(x="Year", y = "Recruit abundance")+
  theme_classic()
```

```{r setup}
ggplot(data=popmod_ests, aes(x=Year, y=log_R_devs)) +
  geom_line()+
  geom_point()+
  labs(x="Year", y = "Recruitment deviations",ylim=c(-0.6,0.6))+
  geom_hline(yintercept=0,col="red")+
  theme_classic()
```

##################################################
```{r setup}
ggplot(data=popmod_ests, aes(x=Year, y=u)) +
  geom_line()+
  geom_point()+labs(x="Year", y = "Exploitation Rate")+
  theme_classic()

```

```{r setup}
ggplot(data=catch, aes(x=Year, y=Catch)) +
  geom_line()+
  geom_point()+
  labs(x="Year", y = "Catch")+
  theme_classic()
#####################################################
```

```{r setup}
ggplot(data=compad, aes(x=Year, y=Est,group=ObsPred)) +
  geom_line(aes(col=ObsPred))+
  geom_point(aes(shape=ObsPred))+
  labs(x="Year", y = "Adult index")+
  theme_classic()
```

```{r setup}
ggplot(data=compjuv, aes(x=Year, y=Est,group=ObsPred)) +
  geom_line(aes(col=ObsPred))+
  geom_point(aes(shape=ObsPred))+
  labs(x="Year", y = "Recruit index")+
  theme_classic()
```
###################################################################
```{r setup}
std<-read.table("examples/class 4/bcrab.std",skip=-1)
std<-std[-1,]
names(std)<-c("index","name","value","std.dev")
```

```{r setup}
log_R_devs  <-which(std$name=="log_R_devs")
log_R_devs  <-std[log_R_devs,]
log_R_devs  <-as.numeric(log_R_devs  [,3])
```

```{r setup}
rr<-dim(log_R_devs)
rpr.c<-as.numeric(levels(log_R_devs[,3]))[as.numeric(log_R_devs[,3])]
sup<-rpr.c+2*as.numeric(levels(log_R_devs[,4]))[as.integer(log_R_devs[,4])]
inf<-rpr.c-2*as.numeric(levels(log_R_devs[,4]))[as.integer(log_R_devs[,4])]
poli<-rbind(cbind(year,sup),cbind(rev(year),rev(inf)))
```

To reproduce example from class 8

```{r setup}
MFLFs=read.delim("examples/class 8/sbft.rep",sep="",header=FALSE)
otherres=read.delim("examples/class 8/other.rep",sep="",header=FALSE)
obsLF=otherres[1:10,]
obsNAL=otherres[11:20,]
#Prep for visualation
midbins=seq(30.5,208.5,2)
ts=10 #number of datasets
obsLFplot=t(rbind(obsLF,midbins,rep(2,length(midbins))))
colnames(obsLFplot)=c(seq(1,ts),"Midbin","Model")
MFLFsplot=t(rbind(MFLFs,midbins,rep(3,length(midbins))))
colnames(MFLFsplot)=c(seq(1,ts),"Midbin","Model")
resLF=data.frame(rbind(MFLFsplot,obsLFplot))
resLF$Model=as.factor(resLF$Model);levels(resLF$Model)= c("Obs","Est")
#Plot observed vs estimated LFs
graphics.off()
for(t in 1:ts){
  resLFsub=resLF[,t]
  resLFsub=data.frame(cbind(resLFsub,resLF[,ts+1],resLF[,ts+2]))
  colnames(resLFsub)=c("Proportion","Midbin","Model")
  resLFsub$Model=as.factor(resLFsub$Model)
  levels(resLFsub$Model) = c("Obs","Est")
  g=ggplot(resLFsub,aes(x=Midbin,y=Proportion,color=Model))+
    geom_line()+
    geom_point(size=4,aes(shape=Model))+
    scale_shape_manual(values=c(16,15))+
    scale_color_manual(values=c("#FF33B5","#660882"))+
    theme_bw(base_size = 20)+
    xlab("Length(midbin)")+
    theme(
      legend.position = c(.95, .95),
      legend.justification = c("right", "top"),
      legend.box.just = "right",
      legend.margin = margin(6, 6, 6, 6)
    )
  print(g)
}
```

```{r setup}
#Grab vonB parameter estimates
vbkest=otherres[21,1]
linf=otherres[22,1]
mLfirst=otherres[23,1]
mLlast=otherres[24,1]
vbkest
linf
mLfirst
mLlast
```
How sensitive is the model to the assumption of number of ages? How sensitive is the model to starting
 values?

